import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
from tqdm import tqdm
import os
from datetime import datetime

from config import Config
from data.dataset import get_data_loaders
from models.custom_cnn import CustomCNN
from models.resnet import ResNetTransfer as ResNetModel
from utils.metrics import MetricsCalculator, print_metrics_summary
from utils.early_stopping import EarlyStopping
from utils.losses import CombinedLoss

# åˆ›å»ºç»“æœä¿å­˜ç›®å½•
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
save_dir = os.path.join('results', timestamp)
os.makedirs(save_dir, exist_ok=True)

# ä¿å­˜è®­ç»ƒé…ç½®
with open(os.path.join(save_dir, 'config.txt'), 'w') as f:
    for key, value in vars(Config).items():
        if not key.startswith('__'):
            f.write(f'{key}: {value}\n')

class LabelSmoothingLoss(nn.Module):
    """æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°"""
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
        
    def forward(self, pred, target):
        pred = pred.log_softmax(dim=-1)
        with torch.no_grad():
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (pred.size(-1) - 1))
            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * pred, dim=-1))

def train_epoch(model, train_loader, criterion, optimizer, device, metrics_calculator):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_labels = []
    all_scores = []
    
    pbar = tqdm(train_loader, desc='Training')
    for inputs, labels in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        scores = torch.softmax(outputs, dim=1)
        _, predicted = outputs.max(1)
        
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_scores.extend(scores.detach().cpu().numpy())
        
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    all_scores = np.array(all_scores)
    
    # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
    metrics = metrics_calculator.calculate_all_metrics(
        all_labels,
        all_preds,
        all_scores
    )
    metrics['loss'] = running_loss / len(train_loader)
    
    # æ·»åŠ åŸå§‹é¢„æµ‹æ•°æ®
    metrics['y_true'] = all_labels
    metrics['y_pred'] = all_preds
    metrics['y_score'] = all_scores
    
    return metrics

def validate(model, val_loader, criterion, device, metrics_calculator):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []
    all_scores = []
    
    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validation')
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            scores = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_scores.extend(scores.cpu().numpy())
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    all_scores = np.array(all_scores)
    
    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    metrics = metrics_calculator.calculate_all_metrics(
        all_labels,
        all_preds,
        all_scores
    )
    metrics['loss'] = running_loss / len(val_loader)
    
    # æ·»åŠ åŸå§‹é¢„æµ‹æ•°æ®
    metrics['y_true'] = all_labels
    metrics['y_pred'] = all_preds
    metrics['y_score'] = all_scores
    
    return metrics

def train_model(model, train_loader, val_loader, num_epochs, device, model_save_path):
    """è®­ç»ƒæ¨¡å‹"""
    # è®¾ç½®æŸå¤±å‡½æ•°
    criterion = CombinedLoss(
        alpha=Config.LOSS['focal_alpha'],
        gamma=Config.LOSS['focal_gamma'],
        smoothing=Config.LOSS['label_smoothing']
    )
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )
    
    # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=num_epochs,
        eta_min=Config.MIN_LR
    )
    
    # è®¾ç½®æ—©åœ
    early_stopping = EarlyStopping(
        patience=Config.EARLY_STOPPING_PATIENCE,
        mode='max'  # ç›‘æ§éªŒè¯å‡†ç¡®ç‡
    )
    
    # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
    metrics_calculator = MetricsCalculator()
    
    best_val_acc = 0.0
    best_model_state = None
    
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        
        # è®­ç»ƒé˜¶æ®µ
        train_metrics = train_epoch(
            model, train_loader, criterion,
            optimizer, device, metrics_calculator
        )
        
        # éªŒè¯é˜¶æ®µ
        val_metrics = validate(
            model, val_loader, criterion,
            device, metrics_calculator
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # æ‰“å°æŒ‡æ ‡
        print('\nTraining Metrics:')
        print_metrics_summary(train_metrics)
        print('\nValidation Metrics:')
        print_metrics_summary(val_metrics)
        
        # ä¿å­˜æ··æ·†çŸ©é˜µ
        metrics_calculator.plot_confusion_matrix(
            val_metrics['confusion_matrix'],
            save_path=os.path.join(save_dir, f'confusion_matrix_epoch_{epoch+1}.png')
        )
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['accuracy'] > best_val_acc:
            best_val_acc = val_metrics['accuracy']
            best_model_state = model.state_dict().copy()
            torch.save(best_model_state, model_save_path)
            print(f"\nBest model saved with validation accuracy: {best_val_acc:.4f}")
        
        # æ£€æŸ¥æ—©åœ
        early_stopping(val_metrics['accuracy'], model)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            # æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€
            model.load_state_dict(best_model_state)
            break
    
    return train_metrics, val_metrics

def evaluate_on_test(model, test_loader, device, metrics_calculator, model_name):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_labels = []
    all_scores = []
    
    print(f"\nè¯„ä¼° {model_name} åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½...")
    
    with torch.no_grad():
        pbar = tqdm(test_loader, desc=f'Testing {model_name}')
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            
            scores = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_scores.extend(scores.cpu().numpy())
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    all_scores = np.array(all_scores)
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    test_metrics = metrics_calculator.calculate_all_metrics(
        all_labels,
        all_preds,
        all_scores
    )
    
    # æ·»åŠ åŸå§‹é¢„æµ‹æ•°æ®
    test_metrics['y_true'] = all_labels
    test_metrics['y_pred'] = all_preds
    test_metrics['y_score'] = all_scores
    
    return test_metrics

def compare_models(cnn_metrics, resnet_metrics, cnn_params, resnet_params, save_dir):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("\n" + "="*80)
    print("                    æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ (Q1 vs Q2)")
    print("="*80)
    
    # å‚æ•°é‡å¯¹æ¯”
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°é‡å¯¹æ¯”:")
    print(f"â”œâ”€ Q1 (è‡ªå®šä¹‰CNN):     {cnn_params['total']:,} å‚æ•° (å…¨éƒ¨å¯è®­ç»ƒ)")
    print(f"â”œâ”€ Q2 (ResNetè¿ç§»å­¦ä¹ ): {resnet_params['total']:,} å‚æ•°")
    print(f"â”‚  â”œâ”€ å¯è®­ç»ƒå‚æ•°:      {resnet_params['trainable']:,} å‚æ•° ({resnet_params['trainable']/resnet_params['total']*100:.2f}%)")
    print(f"â”‚  â””â”€ å†»ç»“å‚æ•°:        {resnet_params['frozen']:,} å‚æ•° ({resnet_params['frozen']/resnet_params['total']*100:.2f}%)")
    
    # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
    metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'ap_class_0', 'ap_class_1', 'ap_class_2']
    metric_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'Normal AP', 'Benign AP', 'Cancer AP']
    
    print(f"\nğŸ“ˆ æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<15} {'Q1 (CNN)':<12} {'Q2 (ResNet)':<14} {'å·®å¼‚':<10} {'ä¼˜åŠ¿'}")
    print("-" * 70)
    
    for metric, name in zip(metrics_to_compare, metric_names):
        cnn_val = cnn_metrics.get(metric, 0)
        resnet_val = resnet_metrics.get(metric, 0)
        diff = resnet_val - cnn_val
        winner = "ResNet" if diff > 0 else "CNN" if diff < 0 else "å¹³å±€"
        
        print(f"{name:<15} {cnn_val:<12.4f} {resnet_val:<14.4f} {diff:+.4f}   {winner}")
    
    # ç±»åˆ«çº§åˆ«åˆ†æ
    print(f"\nğŸ” å„ç±»åˆ«æ€§èƒ½è¯¦ç»†åˆ†æ:")
    class_names = ['Normal', 'Benign', 'Cancer']
    
    for i, class_name in enumerate(class_names):
        print(f"\n{class_name} ç±»åˆ«:")
        cnn_f1 = cnn_metrics.get(f'f1_class_{i}', 0)
        resnet_f1 = resnet_metrics.get(f'f1_class_{i}', 0)
        cnn_auc = cnn_metrics.get(f'auc_class_{i}', 0)
        resnet_auc = resnet_metrics.get(f'auc_class_{i}', 0)
        
        print(f"  F1åˆ†æ•°:  CNN={cnn_f1:.4f}, ResNet={resnet_f1:.4f} (å·®å¼‚: {resnet_f1-cnn_f1:+.4f})")
        print(f"  AUC:     CNN={cnn_auc:.4f}, ResNet={resnet_auc:.4f} (å·®å¼‚: {resnet_auc-cnn_auc:+.4f})")
    
    # è¿ç§»å­¦ä¹ ä¼˜åŠ£åˆ†æ
    print(f"\nğŸ’¡ è¿ç§»å­¦ä¹ åˆ†æ:")
    
    # å‚æ•°æ•ˆç‡
    param_efficiency = (resnet_metrics['accuracy'] / resnet_params['trainable']) / (cnn_metrics['accuracy'] / cnn_params['total'])
    print(f"â”œâ”€ å‚æ•°æ•ˆç‡: ResNetçš„å‚æ•°æ•ˆç‡{'é«˜äº' if param_efficiency > 1 else 'ä½äº'}è‡ªå®šä¹‰CNN {param_efficiency:.2f}å€")
    
    # æ•´ä½“æ€§èƒ½
    overall_better = resnet_metrics['accuracy'] > cnn_metrics['accuracy']
    acc_diff = resnet_metrics['accuracy'] - cnn_metrics['accuracy']
    print(f"â”œâ”€ æ•´ä½“æ€§èƒ½: ResNetå‡†ç¡®ç‡{'é«˜äº' if overall_better else 'ä½äº'}CNN {abs(acc_diff)*100:.2f}ä¸ªç™¾åˆ†ç‚¹")
    
    # ç±»åˆ«åå¥½åˆ†æ
    cancer_performance = resnet_metrics.get('f1_class_2', 0) - cnn_metrics.get('f1_class_2', 0)
    print(f"â”œâ”€ ç™Œç—‡æ£€æµ‹: ResNetåœ¨ç™Œç—‡ç±»åˆ«F1åˆ†æ•°{'ä¼˜äº' if cancer_performance > 0 else 'åŠ£äº'}CNN {abs(cancer_performance):.4f}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'parameter_comparison': {
            'cnn': cnn_params,
            'resnet': resnet_params
        },
        'performance_comparison': {
            'cnn': {k: v for k, v in cnn_metrics.items() if k in metrics_to_compare},
            'resnet': {k: v for k, v in resnet_metrics.items() if k in metrics_to_compare}
        },
        'analysis': {
            'parameter_efficiency': param_efficiency,
            'accuracy_difference': acc_diff,
            'cancer_detection_improvement': cancer_performance
        }
    }
    
    # ä¿å­˜å¯¹æ¯”ç»“æœåˆ°æ–‡ä»¶
    torch.save(comparison_results, os.path.join(save_dir, 'model_comparison.pth'))
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šæ–‡æœ¬
    with open(os.path.join(save_dir, 'comparison_report.txt'), 'w', encoding='utf-8') as f:
        f.write("MTH416 æ·±åº¦å­¦ä¹ é¡¹ç›® - æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š\n")
        f.write("="*60 + "\n\n")
        
        f.write("1. æ¨¡å‹å‚æ•°é‡å¯¹æ¯”\n")
        f.write(f"   Q1 (è‡ªå®šä¹‰CNN): {cnn_params['total']:,} å‚æ•°\n")
        f.write(f"   Q2 (ResNetè¿ç§»å­¦ä¹ ): {resnet_params['trainable']:,} å¯è®­ç»ƒå‚æ•° / {resnet_params['total']:,} æ€»å‚æ•°\n\n")
        
        f.write("2. æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”\n")
        for metric, name in zip(metrics_to_compare, metric_names):
            cnn_val = cnn_metrics.get(metric, 0)
            resnet_val = resnet_metrics.get(metric, 0)
            f.write(f"   {name}: CNN={cnn_val:.4f}, ResNet={resnet_val:.4f}\n")
        
        f.write(f"\n3. è¿ç§»å­¦ä¹ ä¼˜åŠ¿åˆ†æ\n")
        f.write(f"   - å‚æ•°æ•ˆç‡: {param_efficiency:.2f}\n")
        f.write(f"   - å‡†ç¡®ç‡æå‡: {acc_diff*100:+.2f}%\n")
        f.write(f"   - ç™Œç—‡æ£€æµ‹æ”¹è¿›: {cancer_performance:+.4f}\n")
    
    return comparison_results

def count_model_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    return {
        'total': total_params,
        'trainable': trainable_params, 
        'frozen': frozen_params
    }

def main():
    # è®¾ç½®è®¾å¤‡
    device = torch.device(Config.DEVICE)
    print(f"Using device: {device}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = get_data_loaders()
    
    # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
    metrics_calculator = MetricsCalculator()
    
    # ===============================
    # Q1: è®­ç»ƒè‡ªå®šä¹‰CNNæ¨¡å‹
    # ===============================
    print("\nğŸš€ Q1: è®­ç»ƒè‡ªå®šä¹‰CNNæ¨¡å‹...")
    cnn_model = CustomCNN().to(device)
    
    # ç»Ÿè®¡CNNå‚æ•°é‡
    cnn_params = count_model_parameters(cnn_model)
    print(f"\nğŸ“Š è‡ªå®šä¹‰CNNå‚æ•°ç»Ÿè®¡:")
    print(f"â”œâ”€ æ€»å‚æ•°æ•°é‡: {cnn_params['total']:,}")
    print(f"â”œâ”€ å¯è®­ç»ƒå‚æ•°: {cnn_params['trainable']:,}")
    print(f"â””â”€ å†»ç»“å‚æ•°: {cnn_params['frozen']:,}")
    
    cnn_save_path = os.path.join(save_dir, 'cnn_model.pth')
    cnn_train_metrics, cnn_val_metrics = train_model(
        cnn_model, train_loader, val_loader,
        Config.NUM_EPOCHS, device, cnn_save_path
    )
    
    # ===============================
    # Q2: è®­ç»ƒResNetè¿ç§»å­¦ä¹ æ¨¡å‹
    # ===============================
    print("\nğŸš€ Q2: è®­ç»ƒResNetè¿ç§»å­¦ä¹ æ¨¡å‹...")
    resnet_model = ResNetModel().to(device)
    
    # ç»Ÿè®¡ResNetå‚æ•°é‡
    resnet_params = count_model_parameters(resnet_model)
    print(f"\nğŸ“Š ResNetè¿ç§»å­¦ä¹ å‚æ•°ç»Ÿè®¡:")
    print(f"â”œâ”€ æ€»å‚æ•°æ•°é‡: {resnet_params['total']:,}")
    print(f"â”œâ”€ å¯è®­ç»ƒå‚æ•°: {resnet_params['trainable']:,} ({resnet_params['trainable']/resnet_params['total']*100:.2f}%)")
    print(f"â””â”€ å†»ç»“å‚æ•°: {resnet_params['frozen']:,} ({resnet_params['frozen']/resnet_params['total']*100:.2f}%)")
    
    resnet_save_path = os.path.join(save_dir, 'resnet_model.pth')
    resnet_train_metrics, resnet_val_metrics = train_model(
        resnet_model, train_loader, val_loader,
        Config.NUM_EPOCHS, device, resnet_save_path
    )
    
    # ===============================
    # æµ‹è¯•é›†è¯„ä¼°
    # ===============================
    print("\nğŸ” æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    cnn_model.load_state_dict(torch.load(cnn_save_path, map_location=device))
    resnet_model.load_state_dict(torch.load(resnet_save_path, map_location=device))
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    cnn_test_metrics = evaluate_on_test(cnn_model, test_loader, device, metrics_calculator, "è‡ªå®šä¹‰CNN")
    resnet_test_metrics = evaluate_on_test(resnet_model, test_loader, device, metrics_calculator, "ResNetè¿ç§»å­¦ä¹ ")
    
    # ===============================
    # Q3: ç”Ÿæˆæ··æ·†çŸ©é˜µå’ŒPRæ›²çº¿
    # ===============================
    print("\nğŸ“Š Q3: ç”Ÿæˆè¯„ä¼°å›¾è¡¨...")
    
    # CNNæ¨¡å‹å›¾è¡¨
    metrics_calculator.plot_confusion_matrix(
        cnn_test_metrics['confusion_matrix'],
        save_path=os.path.join(save_dir, 'q1_cnn_confusion_matrix_test.png')
    )
    metrics_calculator.plot_pr_curves(
        cnn_test_metrics['y_true'],
        cnn_test_metrics['y_score'],
        save_path=os.path.join(save_dir, 'q1_cnn_precision_recall_test.png')
    )
    
    # ResNetæ¨¡å‹å›¾è¡¨  
    metrics_calculator.plot_confusion_matrix(
        resnet_test_metrics['confusion_matrix'],
        save_path=os.path.join(save_dir, 'q2_resnet_confusion_matrix_test.png')
    )
    metrics_calculator.plot_pr_curves(
        resnet_test_metrics['y_true'],
        resnet_test_metrics['y_score'],
        save_path=os.path.join(save_dir, 'q2_resnet_precision_recall_test.png')
    )
    
    # ===============================
    # Q4: æ¨¡å‹å¯¹æ¯”åˆ†æ
    # ===============================
    print("\nâš–ï¸ Q4: æ¨¡å‹å¯¹æ¯”åˆ†æ...")
    comparison_results = compare_models(
        cnn_test_metrics, resnet_test_metrics, 
        cnn_params, resnet_params, save_dir
    )
    
    # ===============================
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    # ===============================
    final_results = {
        'models': {
            'cnn': {
                'parameters': cnn_params,
                'train_metrics': cnn_train_metrics,
                'val_metrics': cnn_val_metrics,
                'test_metrics': cnn_test_metrics
            },
            'resnet': {
                'parameters': resnet_params, 
                'train_metrics': resnet_train_metrics,
                'val_metrics': resnet_val_metrics,
                'test_metrics': resnet_test_metrics
            }
        },
        'comparison': comparison_results
    }
    
    torch.save(final_results, os.path.join(save_dir, 'final_results.pth'))
    
    # ===============================
    # æœ€ç»ˆæŠ¥å‘Šè¾“å‡º
    # ===============================
    print("\n" + "="*80)
    print("                           æœ€ç»ˆå®éªŒç»“æœæ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ“ˆ Q1 (è‡ªå®šä¹‰CNN) æµ‹è¯•é›†æ€§èƒ½:")
    print_metrics_summary(cnn_test_metrics)
    
    print(f"\nğŸ“ˆ Q2 (ResNetè¿ç§»å­¦ä¹ ) æµ‹è¯•é›†æ€§èƒ½:")
    print_metrics_summary(resnet_test_metrics)
    
    print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"â”œâ”€ æ¨¡å‹æƒé‡: cnn_model.pth, resnet_model.pth")
    print(f"â”œâ”€ æ··æ·†çŸ©é˜µ: *_confusion_matrix_test.png") 
    print(f"â”œâ”€ PRæ›²çº¿: *_precision_recall_test.png")
    print(f"â”œâ”€ å¯¹æ¯”æŠ¥å‘Š: comparison_report.txt")
    print(f"â””â”€ å®Œæ•´ç»“æœ: final_results.pth")

if __name__ == '__main__':
    main() 