import torch
import torch.nn as nn
from datetime import datetime
import os
from tqdm import tqdm

from config import Config
from data.dataset import get_data_loaders
from models.custom_cnn import CustomCNN, count_parameters
from models.resnet import ResNetTransfer, count_parameters as count_resnet_parameters
from utils.metrics import MetricsCalculator, print_metrics_summary

def quick_test():
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿæµ‹è¯•...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(Config.DEVICE)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = get_data_loaders()
    
    # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
    metrics_calculator = MetricsCalculator()
    
    # æµ‹è¯•CNNæ¨¡å‹
    print("\nğŸ“Š æµ‹è¯•è‡ªå®šä¹‰CNNæ¨¡å‹...")
    cnn_model = CustomCNN().to(device)
    cnn_params = count_parameters(cnn_model)
    print(f"CNNæ€»å‚æ•°é‡: {cnn_params:,}")
    
    # æµ‹è¯•ResNetæ¨¡å‹
    print("\nğŸ“Š æµ‹è¯•ResNetè¿ç§»å­¦ä¹ æ¨¡å‹...")
    resnet_model = ResNetTransfer().to(device)
    resnet_params = count_resnet_parameters(resnet_model)
    print(f"ResNetå¯è®­ç»ƒå‚æ•°é‡: {resnet_params:,}")
    
    # å¿«é€Ÿæµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡çš„å‰å‘ä¼ æ’­
    print("\nğŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­...")
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        # CNNå‰å‘ä¼ æ’­
        cnn_outputs = cnn_model(inputs)
        print(f"CNNè¾“å‡ºå½¢çŠ¶: {cnn_outputs.shape}")
        
        # ResNetå‰å‘ä¼ æ’­
        resnet_outputs = resnet_model(inputs)
        print(f"ResNetè¾“å‡ºå½¢çŠ¶: {resnet_outputs.shape}")
        
        break  # åªæµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    
    print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼æ¨¡å‹ç»“æ„æ­£å¸¸ã€‚")
    
    # ç”Ÿæˆå‚æ•°å¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*60)
    print("                  æ¨¡å‹å‚æ•°å¯¹æ¯”")
    print("="*60)
    print(f"Q1 (è‡ªå®šä¹‰CNN)     - æ€»å‚æ•°: {cnn_params:,}")
    print(f"Q2 (ResNetè¿ç§»å­¦ä¹ ) - å¯è®­ç»ƒå‚æ•°: {resnet_params:,}")
    print(f"å‚æ•°æ•ˆç‡æå‡: {cnn_params / resnet_params:.1f}å€")

def simple_evaluation():
    """ç®€å•è¯„ä¼°ç°æœ‰æ¨¡å‹"""
    device = torch.device(Config.DEVICE)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„æ¨¡å‹
    model_path = "best_model.pth"
    if os.path.exists(model_path):
        print(f"ğŸ“‚ æ‰¾åˆ°å·²è®­ç»ƒæ¨¡å‹: {model_path}")
        
        # åŠ è½½æ•°æ®
        _, _, test_loader = get_data_loaders()
        metrics_calculator = MetricsCalculator()
        
        # åˆ›å»ºæ¨¡å‹
        model = CustomCNN().to(device)
        
        try:
            # åŠ è½½æƒé‡
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.eval()
            
            print("ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
            
            all_preds = []
            all_labels = []
            all_scores = []
            
            with torch.no_grad():
                for inputs, labels in tqdm(test_loader, desc='è¯„ä¼°ä¸­'):
                    inputs = inputs.to(device)
                    outputs = model(inputs)
                    scores = torch.softmax(outputs, dim=1)
                    _, predicted = outputs.max(1)
                    
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.numpy())
                    all_scores.extend(scores.cpu().numpy())
            
            import numpy as np
            all_labels = np.array(all_labels)
            all_preds = np.array(all_preds)
            all_scores = np.array(all_scores)
            
            # è®¡ç®—æŒ‡æ ‡
            test_metrics = metrics_calculator.calculate_all_metrics(
                all_labels, all_preds, all_scores
            )
            
            print("\nğŸ“ˆ æµ‹è¯•é›†æ€§èƒ½:")
            print_metrics_summary(test_metrics)
            
            # ç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨
            save_dir = f"results/quick_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            os.makedirs(save_dir, exist_ok=True)
            
            metrics_calculator.plot_confusion_matrix(
                test_metrics['confusion_matrix'],
                save_path=os.path.join(save_dir, 'confusion_matrix.png')
            )
            
            metrics_calculator.plot_pr_curves(
                all_labels, all_scores,
                save_path=os.path.join(save_dir, 'precision_recall.png')
            )
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    else:
        print("âŒ æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶")

if __name__ == "__main__":
    print("MTH416 æ·±åº¦å­¦ä¹ é¡¹ç›® - å¿«é€Ÿæµ‹è¯•å·¥å…·")
    print("="*50)
    
    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    quick_test()
    
    # å°è¯•è¯„ä¼°ç°æœ‰æ¨¡å‹
    simple_evaluation() 